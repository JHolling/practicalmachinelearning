---
title: "Activity Prediction-Machine Learning Project"
author: "Jess Holling"
date: "Feburary 6, 2016"
output: 
  html_document: 
    keep_md: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```
# Project Summary
From the research project "Qualitative Activity Recognition of Weight Lifting Exercizes" by Groupware@LES, six males performed specific Weight Lift Exercises. These males were fitted with four sensors (belt, arm, glove, dumbell) to record exercise movements covering a 0.5 to 2.5 second window. Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 

 * (Class A)- No mistakes & exactly according to the specification
 * (Class B)- Mistake of throwing the elbows to the front 
 * (Class C)- Mistake of lifting the dumbbell only halfway   
 * (Class D)- Mistake of lowering the dumbbell only halfway 
 * (Class E)- Mistake of throwing the hips to the front 

The goal of my project is to take the same weight lifting database and develop a highly accurate prediction model that can determine, via the "classe" variable, how well the exercise was completed (i.e proper-A or which mistake-B/C/D/E).

I will employ two machine learning models: Random Forest & Support Vector Machines (SVM). These models are efficient for large variables & classification analysis. The highest accuracy model on a validating data set will be choosen as "best" model.

This "best" new model will then be applied to the "test" data set consisting of 20 observations without the "classe" a.k.a. exercise quality known and entered to the Course Project Prediction Quiz and only the is code shown below.

# Results
The fitted model for Random Forest gave 99.08% accuracy while SVM gave 99.45% accuracy based on the validation data. Both models are great "fits" exceeding 99% accuracy. The SVM is chosen for having 0.37% less error rate and higher Kappa then Random Forest and will be used for the test data set. 

### Load data and clean data
```{r data load}
set.seed(100)
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(parallel))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(kernlab))
suppressPackageStartupMessages(library(corrplot))

if(!file.exists("pml-training.csv")) #checks if file is download to working directory
{  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                 "pml-training.csv")
   download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                 "pml-testing.csv")}

train <- read.csv("pml-training.csv", na.strings = c("NA", ""))
test <- read.csv("pml-testing.csv") # loading 20 test observations
dim(train)
str(train[1:6,1:12]) #sample of 160 columns with 12 rows

#Removing columns that contain NA Note: these cols also have some calculations too#
noNA<-sapply(train,function(x) sum(is.na(x)))==0
train.noNA <- train[,noNA]
test.noNA<-test[,noNA]

#Remove other columns that will not be used 
train.tidy = train.noNA[,!grepl("^X|raw|cvtd|new|num",names(train.noNA))]
test.tidy =  test.noNA[,!grepl("^X|raw|cvtd|new|num|prob",names(test.noNA))]

train.split<-createDataPartition(y=train.tidy$classe,p=0.75, list=F)
training<-train.tidy[train.split,]   #splitting data set to 75/25 of original
validate<-train.tidy[-train.split,]    #will use to verify model is good fit
```
Looking at the given "train" data set (19622x160), several columns containing "NA"s also have calculations (i.e "var", "mean", etc) at the end of each short window section.  I choose to remove those as well as other non needed columns. 

The variables were adjusted from 160 down to 54. Lastly, this data set was split 75% into "train.tidy" & 25% into "validate". 

### Correlation plot
```{r correlation plot,fig.height=6}
cor.train <- cor(training[, -c(1,54)])
corrplot(cor.train, method=c("color"), bg = "white", addgrid.col = "gray50", tl.cex=.7,
         type="lower", l.col = "black", col = colorRampPalette(c("red","white","blue"))(100))
```

A few of the variables (i.e. yaw_belt & roll_belt) look to be highly correlated (more color = more correlation) and need to be considered in models.

## Fit new models
Adding 10-k fold cross validation to models will test multiple variables across multiple resamples and choose the best overall. This will further increase accuracy and lower overfit. The downside is processing time. So parallel processing has been added for multiply processor cores.

```{r prepare processing controls}
cluster <- makeCluster(detectCores() - 1) #use all but 1 processor for calcs
registerDoParallel(cluster)

#setting for 10 K-fold cross validation & mulit processors
fitControl <- trainControl(method ="cv",number=10, allowParallel = TRUE) 
```

### Random Forest model
```{r random forest model}
fit.rf<-train(classe~.,method="rf" ,data=training, trControl=fitControl) 
confusionMatrix(fit.rf) 
```
The Random Forest model is highly accuracy at 99.29% with a 0.71% error rate on the hold-out data used in the training data set.

Next, the SVM model is to be fit. In addition for SVM, normalizing (via center & scale) the data is important to lower the collinearity/correlation when fitting. The “tuneLength” option will adjust the grid to involve more unique model combinations

### SVM model
```{r SVM model}
fit.svm<-train(classe~.,data=training,method="svmRadial", preProcess=c("center","scale"),
         tuneLength=15,trControl=fitControl)  
confusionMatrix(fit.svm)
stopCluster(cluster) #shuts down parallel processing
```
The SVM fitted model is highly accurate at 99.42% with a 0.58% error rate on the hold-out data used in the training data set. Next we will see if the validate data set shows similar results on the these models.

## Validate new models
```{r validate 2 models}

pred.rf<-predict(fit.rf,validate[,-54])  # Random Forest model predicting
rf.conf<-confusionMatrix(pred.rf,validate$classe)$overall
rf.conf[1:4]

pred.svm<-predict(fit.svm,validate[,-54]) # SVM model predicting
svm.conf<-confusionMatrix(pred.svm,validate$classe)$overall
svm.conf[1:4]
```
The validate data set, split out of the original training set, shows both Random Forest & SVM models are very accurate with 99.08% accuracy (0.92% error rate) and 99.45% accuracy (0.55% error rate) respectively. In addition, SVM has notably higher Kappa & 95% C.I.. So SVM is the best model to use.

### Plot Best Model
```{r model plots,fig.height=2.75}

g<-ggplot(fit.svm) 
g+ggtitle("Cost vs Accuracy-SVM model")

plot(varImp(fit.svm),top=7, main="Most Important Variables per classe -SVM model")
```

Plots show "cost" above 500 doesn't increase accuracy and top seven important variables for the fit SVM model.

# 20 test predictions
```{r 20 predictions}
pred.20<-predict(fit.svm,newdata=test.tidy)
```

# Appendix
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.     Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4VVOPVcvn